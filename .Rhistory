coord_map() +
ggtitle("Succes/Echecs de la campagne par département")
france_cible_ggplot
set.seed(123) ## pour pouvoir le reproduire
learn3 = subset(learn, select=-c(insee.code,f_name,last.name,commune,latitude, longitude,X,Y,revenue,reg,rev.mv,revmv.cible,csp, age.tra, ciblenum))
trainIndex = createDataPartition(learn3$cible,p=0.7, list=FALSE,times=1)
train = learn3[trainIndex,]
valid = learn3[-trainIndex,]
model1=glm(cible~., data=train,family = binomial(logit))
model2=glm(cible ~ 1, data=train,family = binomial(logit))
model2.step=stepAIC(model2, direction = "both", scope=list(upper=model1,lower=model2),trace = FALSE)
model2.step$call
model2_aic=round(model2.step$aic,1)
glm.pred1 <- predict(model2.step, newdata = valid, type = "response")
#matrice de confusion
glm.cm0=table(glm.pred1 > 0.5, valid$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=(glm.cm0)
df.glm.cm0=data.frame(VrVal, PrVal, Y)
df.glm.cm0
ggplot(data = df.glm.cm0, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#erreur de prédiction
err.pred1=round(mean(abs((glm.pred1 > 0.5) - valid$cible), na.rm = T)*100,2)
optCut <- optimalCutoff(valid$cible, glm.pred1, optimiseFor = "misclasserror",returnDiagnostics = TRUE)
opt = optCut$optimalCutoff
opt2 = round(optCut$optimalCutoff*100,2)
#matrice de confusion
gml.mc=table(glm.pred1 > opt, valid$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=gml.mc
df.glm.cm=data.frame(VrVal, PrVal, Y)
df.glm.cm
ggplot(data = df.glm.cm, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#prediction
fitted.results = ifelse(glm.pred1 > opt,1,0)
misClasificError =round(mean(fitted.results != valid$cible)*100,2)
misClasificError
optCut.accuracy=100-misClasificError
plot(predict(model2.step),residuals(model2.step))
abline(h=0,lty=2,col="red")
pr=prediction(glm.pred1, valid$cible)
#prf=performance(pr, measure = "tpr", x.measure = "fpr")
prf<-ROCR::performance(pr,"tpr","fpr")
plot(prf,col="orange")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=2)
# calcul AUC
auc=ROCR::performance(pr, "auc")
auc.pred1=round(auc@y.values[[1]],4)
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, ciblenum, latitude, department, longitude,X,Y,REGION,rev.mv,revmv.cible,csp, age.tra))
trainIndex = createDataPartition(learn_arbres$cible,p=0.7, list=FALSE,times=1)
train_arbre = learn_arbres[trainIndex,]
valid_arbre = learn_arbres[-trainIndex,]
New.tree <- rpart(cible~.,data=train_arbre,method="class")
rpart.plot(New.tree, type = 3, clip.right.labs = FALSE, under = TRUE,fallen.leaves=FALSE)
#rpart.rules(New.tree, cover = TRUE)
plotcp(New.tree)
printcp(New.tree)
tree.pred=predict(New.tree,newdata = valid_arbre ,type="class")
tree.mc=table(tree.pred, valid_arbre$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=tree.mc
df.tree.mc=data.frame(VrVal, PrVal, Y)
ggplot(data = df.tree.mc, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
pred = prediction(as.numeric(tree.pred), as.numeric(valid_arbre$cible))
prf2<-ROCR::performance(pred,"tpr","fpr")
plot(prf2,col="hotpink", lwd=2, main="Courbe ROC")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=2)
auc=ROCR::performance(pred, "auc")
tree.auc=round(auc@y.values[[1]],4)
New.tree2 = rpart(cible~.,data=train_arbre,method="class",control = rpart.control(minsplit = 2, cp = 0.0001))
tree.pred2 = predict(New.tree2,newdata = valid_arbre ,type="class")
#matrice de confusion
tree.mc2=table(tree.pred2, valid_arbre$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=tree.mc2
df.tree.mc2=data.frame(VrVal, PrVal, Y)
ggplot(data = df.tree.mc2, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#erreur de prédiction
tree.err2=round((tree.mc2[1,2]+tree.mc2[2,1])/sum(tree.mc2[,])*100,2)
printcp(New.tree2)
mincp = New.tree2$cptable[which.min(New.tree2$cptable[,"xerror"]),"CP"]
plotcp(New.tree2)
final.tree <- rpart(cible~.,data=train_arbre,method="class",control = rpart.control(minsplit = 2, cp = mincp))
tree.pred3 <- predict(final.tree,newdata = valid_arbre ,type="class")
#matrice de confusion
tree.mc3=table(tree.pred3, valid_arbre$cible)
tree.mc3
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=tree.mc3
df.tree.mc3=data.frame(VrVal, PrVal, Y)
ggplot(data = df.tree.mc3, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#erreur de prédiction
tree.err3=round((tree.mc3[1,2]+tree.mc3[2,1])/sum(tree.mc3[,])*100,2)
tree.pred3 <- predict(final.tree,newdata = valid_arbre ,type="class")
#matrice de confusion
tree.mc3=table(tree.pred3, valid_arbre$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=tree.mc3
df.tree.mc3=data.frame(VrVal, PrVal, Y)
ggplot(data = df.tree.mc3, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#erreur de prédiction
tree.err3=round((tree.mc3[1,2]+tree.mc3[2,1])/sum(tree.mc3[,])*100,2)
knitr::opts_chunk$set(echo = FALSE,message=FALSE, warning=FALSE)
# Packages utilisés
library(caret)
library(tidyverse)
#library(gridExtra)
library(knitr)
library(plotly)
library(ggplot2)
library(VIM)
library("GGally")
library("ggmap")
#library(maps)
#library(tmap)
library(mapdata)
library(mapproj)
library(sf)
library(raster)
library(spData)
library(MASS)
library(ROCR)
library(rpart)
library(rpart.plot)
library(InformationValue)
library(randomForest)
library(Matrix)
library(xgboost)
library(readr)
library(stringr)
library(car)
library(data.table)
library(mlr)
#library(DiagrammeR)
# parametrage des graphiques
theme_set=(theme_bw()+theme(plot.title=element_text(hjust=0.5,size=14,face="bold"),plot.subtitle=element_text(hjust=0.5,size=12) ,axis.title=element_text(size=10)))
#chargement des jeux de données d'entrainement et test
learn = read.csv("projet-app-13-learn.csv",header = TRUE,encoding = "UTF-8")
test = read.csv("projet-app-13-test.csv",header = TRUE,encoding = "UTF-8")
# chargement des données géographiques
cities_gps = read.csv("cities-gps.csv",header = TRUE,encoding = "UTF-8")
cities_population = read.csv("cities-population.csv",header = TRUE,encoding = "UTF-8")
dept = read.csv("departments.csv",header = TRUE,encoding = "UTF-8")
regions = read.csv("regions.csv",header = TRUE,encoding = "UTF-8")
# renommage de variables
learn=learn %>% rename(categorie = catégorie)
test=test %>% rename(categorie = catégorie)
#conversion variables en factor
learn$reg = as.factor(learn$reg)
learn$categorie = as.factor(learn$categorie)
test$reg = as.factor(test$reg)
test$categorie = as.factor(test$categorie)
# retraitement variable cible
learn$cible= as.character(learn$cible)
learn$cible[learn$cible == "failure"]=FALSE
learn$cible[learn$cible == "success"]=TRUE
learn$cible=as.logical(learn$cible)
learn$ciblenum[learn$cible == "FALSE"]=-1
learn$ciblenum[learn$cible == "TRUE"]=1
learn$ciblenum=as.numeric(learn$ciblenum)
learn=merge(x=learn,y=cities_gps,by.x = "insee.code",by.y="id")
learn=merge(x=learn,y=cities_population,by.x = "insee.code",by.y="id")
learn=merge(x=learn,y=regions,by.x = "reg",by.y="id")
test=merge(x=test,y=cities_gps,by.x = "insee.code",by.y="id")
test=merge(x=test,y=cities_population,by.x = "insee.code",by.y="id")
test=merge(x=test,y=regions,by.x = "reg",by.y="id")
learn=learn %>% rename(REGION = name)
test=test %>% rename(REGION = name)
learn.row=dim(learn)[1]
#colSums(is.na(learn))
revlNA=sum(is.na(learn$revenue),1)-1
txrevlNA=round((revlNA/nrow(learn)*100))
matrixplot(learn)
#colSums(is.na(test))
revtNA=sum(is.na(test$revenue),1)-1
txrevtNA=round((revtNA/nrow(test)*100))
learn$rev.mv="Valeur"
learn$rev.mv[is.na(learn$revenue)]="NA"
ggplot(learn, aes(x = rev.mv, fill = cible)) +
labs(title = "Répartition du succès/échec de la cible",
x = "Valeur de la variable revenu", y = "Nombre observé",
fill = "Cible", subtitle = "en fonction de la disponibilité de la donnée revenu") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès")) +
geom_bar(col = "black")
#variable composée revenue / cible
learn$revmv.cible="NA"
learn$revmv.cible[learn$rev.mv=="NA" & learn$cible=="FALSE"]="revNA-échec"
learn$revmv.cible[learn$rev.mv=="NA" & learn$cible=="TRUE"]="revNA-succès"
learn$revmv.cible[learn$rev.mv=="Valeur" & learn$cible=="FALSE"]="revVAL-échec"
learn$revmv.cible[learn$rev.mv=="Valeur" & learn$cible=="TRUE"]="revVAL-succès"
# tranches d'age
learn$age.tra[learn$age>=0 & learn$age<=10]="0-10"
learn$age.tra[learn$age>10 & learn$age<=20]="11-20"
learn$age.tra[learn$age>20 & learn$age<=30]="21-30"
learn$age.tra[learn$age>30 & learn$age<=40]="31-40"
learn$age.tra[learn$age>40 & learn$age<=50]="41-50"
learn$age.tra[learn$age>50 & learn$age<=60]="51-60"
learn$age.tra[learn$age>60 & learn$age<=70]="61-70"
learn$age.tra[learn$age>70 & learn$age<=80]="71-80"
learn$age.tra[learn$age>80 & learn$age<=90]="81-90"
learn$age.tra[learn$age>90] ="90-100"
learn$csp[learn$categorie==1]=" 1-Agriculteurs"
learn$csp[learn$categorie==2]=" 2-Artisans, commerçants, chef d'entp"
learn$csp[learn$categorie==3]=" 3-Cadres"
learn$csp[learn$categorie==4]=" 4-Prof. intermédiaires"
learn$csp[learn$categorie==5]=" 5-Empl. qualifiés"
learn$csp[learn$categorie==6]=" 6-Empl. non qualifiés"
learn$csp[learn$categorie==7]=" 7-Ouvr. qualifiés"
learn$csp[learn$categorie==8]=" 8-Ouvr. non qualifiés"
learn$csp[learn$categorie==9]=" 9-Non déterminé"
learn$csp[learn$categorie==10]="10-Etudiants"
learn$csp[learn$categorie==11]="11-Chômeurs"
learn$csp[learn$categorie==12]="12-Inactifs"
learn$csp[learn$categorie==13]="13-Retraités"
p1=ggplot(learn, aes(x = revmv.cible, fill = age.tra)) +
labs(title = "Répartition de la variable composée : \nrésultat campagne / disponibilité revenu ",
x = "Valeur de la variable revenu/cible campagne", y = "Nombre d'observations",
fill = "Age", subtitle = "en fonction de l'age") +
geom_bar(col = "black")
p2=ggplot(learn, aes(x = revmv.cible, fill = csp)) +
labs(
x = "Valeur de la variable revenu/cible campagne", y = "Nombre d'observations",
fill = "CSP", subtitle = "en fonction de la CSP") +
geom_bar(col = "black")
p3=ggplot(learn, aes(x = revmv.cible, fill = sex)) +
labs(
x = "Valeur de la variable revenu/cible campagne", y = "Nombre d'observations",
fill = "Sexe", subtitle = "en fonction du sexe") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Femme","Homme"))+
geom_bar(col = "black")
p4=ggplot(learn, aes(x = revmv.cible, fill = REGION)) +
labs(
x = "Valeur de la variable revenu/cible campagne", y = "Nombre d'observations",
fill = "Région", subtitle = "en fonction de la région") +
geom_bar(col = "black")
p5=ggplot(learn, aes(x = revmv.cible, fill = city.type)) +
labs(
x = "variable revenu/cible campagne", y = "Nombre observé",
fill = "type de ville", subtitle = "en fonction du type de ville") +
geom_bar(col = "black")
p1 + theme(axis.text.x = element_text(angle = 90))
p2 + theme(axis.text.x = element_text(angle = 90))
p3 + theme(axis.text.x = element_text(angle = 90))
p4 + theme(axis.text.x = element_text(angle = 90))
p5 + theme(axis.text.x = element_text(angle = 90))
#fichier d'apprentissage
l.cibleT=sum(learn$cible=="TRUE")
l.cibleF=sum(learn$cible=="FALSE")
l.txcibleT=round((l.cibleT/nrow(learn)*100))
l.txcibleF=round((l.cibleF/nrow(learn)*100))
ggplot(learn,aes(cible)) + geom_bar(col = "black", fill = "lightskyblue")+labs(title = "Répartition du succès/échec",y = "Nombre d'observations")
# effectif femmes
l.nrowF=sum(learn$sex=="Female")
l.txF=round((l.nrowF/nrow(learn)*100))
# effectif hommes
l.nrowM=sum(learn$sex=="Male")
l.txM=round((l.nrowM/nrow(learn)*100))
# taux de succès femmes
l.nrowsucF=learn %>% group_by(sex)  %>% count(cible) %>%
filter(cible=="TRUE" & sex=="Female")
l.txsucF=round((l.nrowsucF[3]/l.nrowF)*100)
# taux de succès hommes
l.nrowsucM=learn %>% group_by(sex)  %>% count(cible) %>%
filter(cible=="TRUE" & sex=="Male")
l.txsucM=round((l.nrowsucM[3]/l.nrowM)*100)
l.txechM=100-l.txsucM
p6=ggplot(learn, aes(x = sex, fill= cible)) +
labs(title = "Répartition du succès/échec",
x = "Sexe", y = "Nombre d'observations",
subtitle = "en fonction du sexe") +
scale_x_discrete(labels=c("Femme","Homme")) +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès"))+
geom_bar(col = "black",position = "dodge")
p6
p7=ggplot(learn, aes(x = age, fill= cible)) +
labs(title = "Répartition du succès/échec",
x = "Age", y = "Nombre d'observations",
subtitle = "en fonction de l'age") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès")) +
geom_bar(col = "black",position = "dodge")
p7
p8=ggplot(learn, aes(x = csp, fill= cible)) +
labs(title = "Répartition du succès/échec",
x = "CSP", y = "Nombre d'observations",
subtitle = "en fonction de la CSP") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès"))+
geom_bar(col = "black",position = "dodge")+coord_flip()
p8
p9=ggplot(learn, aes(x = city.type, fill= cible)) +
labs(title = "Répartition du succès/échec",
x = "Type de ville", y = "Nombre d'observations",
subtitle = "en fonction du type de ville") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès"))+
geom_bar(col = "black",position = "dodge")+coord_flip()
p9
p10=ggplot(learn, aes(x=REGION,fill= cible)) + labs(title = "Répartition du succès/échec",x = "Région", y = "Nombre d'observations",subtitle = "en fonction des régions") + scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès")) + geom_bar(col = "black",position = "dodge")+ coord_flip()
p10
department_ciblenum=learn  %>% group_by(department) %>%
summarize(ciblenum=mean(ciblenum))%>% inner_join(dept,by=c("department"="id"))
cartefrance=as_tibble(map_data("france"))
cartefrance=cartefrance %>% mutate(region=str_to_upper(region))
broken_names=list("VAL-DOISE"="VAL-D'OISE", "COTES-DARMOR"="COTES-D'ARMOR", "COTE-DOR"="COTE-D'OR",
"CORSE DU SUD"="CORSE-DU-SUD")
for(bn in names(broken_names)) {
cartefrance=cartefrance %>% mutate(region=ifelse(region==bn,broken_names[[bn]],region))
}
department_ciblenum$REGION=as.character(department_ciblenum$name)
cartefrance=cartefrance %>% left_join(department_ciblenum,by=c("region"="REGION"))
france_cible_ggplot= ggplot(cartefrance,aes(x=long,y=lat)) +
geom_polygon(aes(group=group,fill=ciblenum),color=grey(0.75)) +
labs(x="", y="", fill="valeur cible moyenne" )+
scale_fill_continuous(type="viridis") +
coord_map() +
ggtitle("Succes/Echecs de la campagne par département")
france_cible_ggplot
set.seed(123) ## pour pouvoir le reproduire
learn3 = subset(learn, select=-c(insee.code,f_name,last.name,commune,latitude, longitude,X,Y,revenue,reg,rev.mv,revmv.cible,csp, age.tra, ciblenum))
trainIndex = createDataPartition(learn3$cible,p=0.7, list=FALSE,times=1)
train = learn3[trainIndex,]
valid = learn3[-trainIndex,]
model1=glm(cible~., data=train,family = binomial(logit))
model2=glm(cible ~ 1, data=train,family = binomial(logit))
model2.step=stepAIC(model2, direction = "both", scope=list(upper=model1,lower=model2),trace = FALSE)
model2.step$call
model2_aic=round(model2.step$aic,1)
glm.pred1 <- predict(model2.step, newdata = valid, type = "response")
#matrice de confusion
glm.cm0=table(glm.pred1 > 0.5, valid$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=(glm.cm0)
df.glm.cm0=data.frame(VrVal, PrVal, Y)
df.glm.cm0
ggplot(data = df.glm.cm0, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#erreur de prédiction
err.pred1=round(mean(abs((glm.pred1 > 0.5) - valid$cible), na.rm = T)*100,2)
optCut <- optimalCutoff(valid$cible, glm.pred1, optimiseFor = "misclasserror",returnDiagnostics = TRUE)
opt = optCut$optimalCutoff
opt2 = round(optCut$optimalCutoff*100,2)
#matrice de confusion
gml.mc=table(glm.pred1 > opt, valid$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=gml.mc
df.glm.cm=data.frame(VrVal, PrVal, Y)
df.glm.cm
ggplot(data = df.glm.cm, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#prediction
fitted.results = ifelse(glm.pred1 > opt,1,0)
misClasificError =round(mean(fitted.results != valid$cible)*100,2)
misClasificError
optCut.accuracy=100-misClasificError
plot(predict(model2.step),residuals(model2.step))
abline(h=0,lty=2,col="red")
pr=prediction(glm.pred1, valid$cible)
#prf=performance(pr, measure = "tpr", x.measure = "fpr")
prf<-ROCR::performance(pr,"tpr","fpr")
plot(prf,col="orange")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=2)
# calcul AUC
auc=ROCR::performance(pr, "auc")
auc.pred1=round(auc@y.values[[1]],4)
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, ciblenum, latitude, department, longitude,X,Y,REGION,rev.mv,revmv.cible,csp, age.tra))
trainIndex = createDataPartition(learn_arbres$cible,p=0.7, list=FALSE,times=1)
train_arbre = learn_arbres[trainIndex,]
valid_arbre = learn_arbres[-trainIndex,]
New.tree <- rpart(cible~.,data=train_arbre,method="class")
rpart.plot(New.tree, type = 3, clip.right.labs = FALSE, under = TRUE,fallen.leaves=FALSE)
#rpart.rules(New.tree, cover = TRUE)
plotcp(New.tree)
printcp(New.tree)
tree.pred=predict(New.tree,newdata = valid_arbre ,type="class")
tree.mc=table(tree.pred, valid_arbre$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=tree.mc
df.tree.mc=data.frame(VrVal, PrVal, Y)
ggplot(data = df.tree.mc, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
pred = prediction(as.numeric(tree.pred), as.numeric(valid_arbre$cible))
prf2<-ROCR::performance(pred,"tpr","fpr")
plot(prf2,col="hotpink", lwd=2, main="Courbe ROC")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=2)
auc=ROCR::performance(pred, "auc")
tree.auc=round(auc@y.values[[1]],4)
New.tree2 = rpart(cible~.,data=train_arbre,method="class",control = rpart.control(minsplit = 2, cp = 0.0001))
tree.pred2 = predict(New.tree2,newdata = valid_arbre ,type="class")
#matrice de confusion
tree.mc2=table(tree.pred2, valid_arbre$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=tree.mc2
df.tree.mc2=data.frame(VrVal, PrVal, Y)
ggplot(data = df.tree.mc2, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#erreur de prédiction
tree.err2=round((tree.mc2[1,2]+tree.mc2[2,1])/sum(tree.mc2[,])*100,2)
printcp(New.tree2)
mincp = New.tree2$cptable[which.min(New.tree2$cptable[,"xerror"]),"CP"]
plotcp(New.tree2)
final.tree <- rpart(cible~.,data=train_arbre,method="class",control = rpart.control(minsplit = 2, cp = mincp))
tree.pred3 <- predict(final.tree,newdata = valid_arbre ,type="class")
#matrice de confusion
tree.mc3=table(tree.pred3, valid_arbre$cible)
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=tree.mc3
df.tree.mc3=data.frame(VrVal, PrVal, Y)
ggplot(data = df.tree.mc3, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#erreur de prédiction
tree.err3=round((tree.mc3[1,2]+tree.mc3[2,1])/sum(tree.mc3[,])*100,2)
barplot(final.tree$variable.importance, main="Arbre de décision : Importance des variables",
col = "cyan",las=2)
pred3 = prediction(as.numeric(tree.pred3), as.numeric(valid_arbre$cible))
prf3=ROCR::performance(pred3,"tpr","fpr")
plot(prf3,col="hotpink", lwd=2, main="Courbe ROC")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=2)
auc2=ROCR::performance(pred3,"auc")
tree2.auc=round(auc2@y.values[[1]],4)
set.seed(1234)
train_arbre$cible[train_arbre$cible == "FALSE"]=0
train_arbre$cible[train_arbre$cible == "TRUE"]=1
train_arbre$cible=as.factor(train_arbre$cible)
#random forest standard
rforest1=randomForest(cible~.,data=train_arbre,na.action=na.omit)
#random forest ntree=2500 mtry=2
rforest2=randomForest(cible~.,data=train_arbre,ntree = 2500,mtry = 2, na.action=na.omit)
#random forest ntree=3500 mtry=2
rforest3=randomForest(cible~.,data=train_arbre,ntree = 3500,mtry = 2, na.action=na.omit)
#random forest ntree=5000 mtry=2
rforest4=randomForest(cible~.,data=train_arbre,ntree = 5000,mtry = 2, na.action=na.omit)
print(rforest1)
#analyse graphique pour choisir ntree
par(mfrow = c(1,3))
plot(rforest1$err.rate[,1],type='l',xlab="nombre d'arbres",ylab="erreur Out Of Bag",main="Taux d'erreur et taille de la foret")
plot(rforest2$err.rate[,1],type='l',xlab="nombre d'arbres",ylab="erreur Out Of Bag")
#plot(rforest3$err.rate[,1],type='l',xlab="nombre d'arbres",ylab="erreur Out Of Bag")
plot(rforest4$err.rate[,1],type='l',xlab="nombre d'arbres",ylab="erreur Out Of Bag")
#erreur oob
rf1.oob=round(rforest1$err.rate[500,1]*100,2)
rf2.oob=round(rforest2$err.rate[2500,1]*100,2)
rf3.oob=round(rforest3$err.rate[3500,1]*100,2)
rf4.oob=round(rforest4$err.rate[5000,1]*100,2)
#random forest ntree=5000 mtry=1
rforest5=randomForest(cible~.,data=train_arbre,ntree = 5000,mtry = 1, na.action=na.omit)
#random forest ntree=5000 mtry=4
rforest6=randomForest(cible~.,data=train_arbre,ntree = 5000,mtry = 4, na.action=na.omit)
#random forest ntree=5000 mtry=6
rforest7=randomForest(cible~.,data=train_arbre,ntree = 5000,mtry = 6, na.action=na.omit)
rf5.oob=round(rforest5$err.rate[5000,1]*100,2)
rf6.oob=round(rforest6$err.rate[5000,1]*100,2)
rf7.oob=round(rforest7$err.rate[5000,1]*100,2)
print(rforest6)
barplot(t(rforest6$importance),las=2, col="cyan",ylim=c(0,800), main="Random Forest : Importance des variables")
# prédiction
valid_arbre$cible[valid_arbre$cible == "FALSE"]=0
valid_arbre$cible[valid_arbre$cible == "TRUE"]=1
valid_arbre$cible=as.factor(valid_arbre$cible)
#matrice de confusion
rforest6.pred=predict(rforest6,newdata=valid_arbre)
rf6.mc=table(rforest6.pred,valid_arbre$cible)
rf6.mc
VrVal=factor(c("Echec", "Echec", "Succès", "Succès"))
PrVal=factor(c("Echec", "Succès", "Echec", "Succès"))
Y=rf6.mc
df.rf6.mc=data.frame(VrVal, PrVal, Y)
ggplot(data = df.rf6.mc, mapping = aes(x = VrVal, y = PrVal)) +
geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
labs(x = "Vraies Valeurs", y = "Valeurs prédites") +
theme_bw()
#erreur de prédiction
#rf6.pred = prediction(as.numeric(rforest6.pred), as.numeric(valid_arbre$cible))
rf6.err=round((rf6.mc[1,2]+rf6.mc[2,1])/sum(rf6.mc[,])*100,2)
