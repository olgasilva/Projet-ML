theme_set=(theme_bw()+theme(plot.title=element_text(hjust=0.5,size=14,face="bold"),plot.subtitle=element_text(hjust=0.5,size=12) ,axis.title=element_text(size=10)))
#chargement des jeux de données d'entrainement et test
learn = read.csv("projet-app-13-learn.csv",header = TRUE,encoding = "UTF-8")
test = read.csv("projet-app-13-test.csv",header = TRUE,encoding = "UTF-8")
# chargement des données géographiques
cities_gps = read.csv("cities-gps.csv",header = TRUE,encoding = "UTF-8")
cities_population = read.csv("cities-population.csv",header = TRUE,encoding = "UTF-8")
dept = read.csv("departments.csv",header = TRUE,encoding = "UTF-8")
regions = read.csv("regions.csv",header = TRUE,encoding = "UTF-8")
# renommage de variable
learn=learn %>% rename(categorie = catégorie)
test=test %>% rename(categorie = catégorie)
#conversion variables en factor
learn$reg = as.factor(learn$reg)
learn$categorie = as.factor(learn$categorie)
test$reg = as.factor(test$reg)
test$categorie = as.factor(test$categorie)
# retraitement variable cible
learn$cible= as.character(learn$cible)
learn$cible[learn$cible == "failure"]=FALSE
learn$cible[learn$cible == "success"]=TRUE
learn$cible=as.logical(learn$cible)
learn$ciblenum[learn$cible == "FALSE"]=-1
learn$ciblenum[learn$cible == "TRUE"]=1
learn$ciblenum=as.numeric(learn$ciblenum)
learn=merge(x=learn,y=cities_gps,by.x = "insee.code",by.y="id")
learn=merge(x=learn,y=cities_population,by.x = "insee.code",by.y="id")
learn=merge(x=learn,y=regions,by.x = "reg",by.y="id")
test=merge(x=test,y=cities_gps,by.x = "insee.code",by.y="id")
test=merge(x=test,y=cities_population,by.x = "insee.code",by.y="id")
test=merge(x=test,y=regions,by.x = "reg",by.y="id")
learn=learn %>% rename(REGION = name)
test=test %>% rename(REGION = name)
learn.row=dim(learn)[1]
#colSums(is.na(learn))
revlNA=sum(is.na(learn$revenue),1)-1
txrevlNA=round((revlNA/nrow(learn)*100))
matrixplot(learn)
#colSums(is.na(test))
revtNA=sum(is.na(test$revenue),1)-1
txrevtNA=round((revtNA/nrow(test)*100))
learn$rev.mv="Valeur"
learn$rev.mv[is.na(learn$revenue)]="NA"
ggplot(learn, aes(x = rev.mv, fill = cible)) +
labs(title = "Répartition du succès/échec de la cible",
x = "Valeur de la variable revenu", y = "Nombre observé",
fill = "Cible", subtitle = "en fonction de la disponibilité de la donnée revenu") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès")) +
geom_bar(col = "black")
#variable composée revenue / cible
learn$revmv.cible="NA"
learn$revmv.cible[learn$rev.mv=="NA" & learn$cible=="FALSE"]="revNA-échec"
learn$revmv.cible[learn$rev.mv=="NA" & learn$cible=="TRUE"]="revNA-succès"
learn$revmv.cible[learn$rev.mv=="Valeur" & learn$cible=="FALSE"]="revVAL-échec"
learn$revmv.cible[learn$rev.mv=="Valeur" & learn$cible=="TRUE"]="revVAL-succès"
# tranches d'age
learn$age.tra[learn$age>=0 & learn$age<=10]="0-10"
learn$age.tra[learn$age>10 & learn$age<=20]="11-20"
learn$age.tra[learn$age>20 & learn$age<=30]="21-30"
learn$age.tra[learn$age>30 & learn$age<=40]="31-40"
learn$age.tra[learn$age>40 & learn$age<=50]="41-50"
learn$age.tra[learn$age>50 & learn$age<=60]="51-60"
learn$age.tra[learn$age>60 & learn$age<=70]="61-70"
learn$age.tra[learn$age>70 & learn$age<=80]="71-80"
learn$age.tra[learn$age>80 & learn$age<=90]="81-90"
learn$age.tra[learn$age>90] ="90-100"
learn$csp[learn$categorie==1]=" 1-Agriculteurs"
learn$csp[learn$categorie==2]=" 2-Artisans, commerçants, chef d'entp"
learn$csp[learn$categorie==3]=" 3-Cadres"
learn$csp[learn$categorie==4]=" 4-Prof. intermédiaires"
learn$csp[learn$categorie==5]=" 5-Empl. qualifiés"
learn$csp[learn$categorie==6]=" 6-Empl. non qualifiés"
learn$csp[learn$categorie==7]=" 7-Ouvr. qualifiés"
learn$csp[learn$categorie==8]=" 8-Ouvr. non qualifiés"
learn$csp[learn$categorie==9]=" 9-Non déterminé"
learn$csp[learn$categorie==10]="10-Etudiants"
learn$csp[learn$categorie==11]="11-Chômeurs"
learn$csp[learn$categorie==12]="12-Inactifs"
learn$csp[learn$categorie==13]="13-Retraités"
p1=ggplot(learn, aes(x = revmv.cible, fill = age.tra)) +
labs(title = "Répartition du succès/échec et de la disponibilité de revenu ",
x = "Valeur de la variable revenu/cible campagne", y = "Nombre d'observations",
fill = "Age", subtitle = "en fonction de l'age") +
geom_bar(col = "black")
p2=ggplot(learn, aes(x = revmv.cible, fill = csp)) +
labs(
x = "Valeur de la variable revenu/cible campagne", y = "Nombre d'observations",
fill = "CSP", subtitle = "en fonction de la CSP") +
geom_bar(col = "black")
p3=ggplot(learn, aes(x = revmv.cible, fill = sex)) +
labs(
x = "Valeur de la variable revenu/cible campagne", y = "Nombre d'observations",
fill = "Sexe", subtitle = "en fonction du sexe") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Femme","Homme"))+
geom_bar(col = "black")
p4=ggplot(learn, aes(x = revmv.cible, fill = REGION)) +
labs(
x = "Valeur de la variable revenu/cible campagne", y = "Nombre d'observations",
fill = "Région", subtitle = "en fonction de la région") +
geom_bar(col = "black")
p5=ggplot(learn, aes(x = revmv.cible, fill = city.type)) +
labs(
x = "variable revenu/cible campagne", y = "Nombre observé",
fill = "type de ville", subtitle = "en fonction du type de ville") +
geom_bar(col = "black")
p1
p2
p3
p4
p5
#fichier d'apprentissage
l.cibleT=sum(learn$cible=="TRUE")
l.cibleF=sum(learn$cible=="FALSE")
l.txcibleT=round((l.cibleT/nrow(learn)*100))
l.txcibleF=round((l.cibleF/nrow(learn)*100))
# effectif femmes
l.nrowF=sum(learn$sex=="Female")
l.txF=round((l.nrowF/nrow(learn)*100))
# effectif hommes
l.nrowM=sum(learn$sex=="Male")
l.txM=round((l.nrowM/nrow(learn)*100))
# taux de succès femmes
l.nrowsucF=learn %>% group_by(sex)  %>% count(cible) %>%
filter(cible=="TRUE" & sex=="Female")
l.txsucF=round((l.nrowsucF[3]/l.nrowF)*100)
# taux de succès hommes
l.nrowsucM=learn %>% group_by(sex)  %>% count(cible) %>%
filter(cible=="TRUE" & sex=="Male")
l.txsucM=round((l.nrowsucM[3]/l.nrowM)*100)
l.txechM=100-l.txsucM
p6=ggplot(learn, aes(x = sex, fill= cible)) +
labs(title = "Répartition du succès/échec",
x = "Sexe", y = "Nombre d'observations",
subtitle = "en fonction du sexe") +
scale_x_discrete(labels=c("Femme","Homme")) +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès"))+
geom_bar(col = "black")
p6
p7=ggplot(learn, aes(x = age, fill= cible)) +
labs(title = "Répartition du succès/échec",
x = "Age", y = "Nombre d'observations",
subtitle = "en fonction de l'age") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès")) +
geom_bar(col = "black")
p7
p8=ggplot(learn, aes(x = csp, fill= cible)) +
labs(title = "Répartition du succès/échec",
x = "CSP", y = "Nombre d'observations",
subtitle = "en fonction de la CSP") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès"))+
geom_bar(col = "black")+coord_flip()
p8
p8=ggplot(learn, aes(x = city.type, fill= cible)) +
labs(title = "Répartition du succès/échec",
x = "Type de ville", y = "Nombre d'observations",
subtitle = "en fonction du type de ville") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès"))+
geom_bar(col = "black")+coord_flip()
p8
p9=ggplot(learn, aes(x = REGION, fill= cible)) +
labs(title = "Répartition du succès/échec",
x = "Région", y = "Nombre d'observations",
subtitle = "en fonction des régions") +
scale_fill_manual(values=c("brown2","lightskyblue"),label=c("Echec","Succès"))+
geom_bar(col = "black")+coord_flip()
p9
department_ciblenum=learn  %>% group_by(department) %>%
summarize(ciblenum=mean(ciblenum))%>% inner_join(dept,by=c("department"="id"))
cartefrance=as_tibble(map_data("france"))
cartefrance=cartefrance %>% mutate(region=str_to_upper(region))
broken_names=list("VAL-DOISE"="VAL-D'OISE", "COTES-DARMOR"="COTES-D'ARMOR", "COTE-DOR"="COTE-D'OR",
"CORSE DU SUD"="CORSE-DU-SUD")
for(bn in names(broken_names)) {
cartefrance=cartefrance %>% mutate(region=ifelse(region==bn,broken_names[[bn]],region))
}
department_ciblenum$REGION=as.character(department_ciblenum$name)
cartefrance=cartefrance %>% left_join(department_ciblenum,by=c("region"="REGION"))
france_cible_ggplot= ggplot(cartefrance,aes(x=long,y=lat)) +
geom_polygon(aes(group=group,fill=ciblenum),color=grey(0.75)) +
labs(x="", y="", fill="valeur cible moyenne" )+
scale_fill_continuous(type="viridis") +
coord_map() +
ggtitle("Succes/Echecs de la campagne par département")
france_cible_ggplot
set.seed(123) ## pour pouvoir le reproduire
learn3 = subset(learn, select=-c(insee.code,f_name,last.name,commune, department, latitude, longitude,X,Y,reg,revenue))
trainIndex = createDataPartition(learn3$cible,p=0.7, list=FALSE,times=1)
train = learn3[trainIndex,]
valid = learn3[-trainIndex,]
model1=glm(cible~., data=train,family = binomial(logit))
model2=glm(cible ~ 1, data=train,family = binomial(logit))
model2.step=stepAIC(model2, direction = "both", scope=list(upper=model1,lower=model2),trace = FALSE)
summary(model2.step)
glm.pred1 <- predict(model2.step, newdata = valid, type = "response")
table(glm.pred1 > 0.5, valid$cible)
mean(abs((glm.pred1 > 0.5) - valid$cible), na.rm = T)
optCut <- optimalCutoff(valid$cible, glm.pred1, optimiseFor = "misclasserror",returnDiagnostics = TRUE)
optCut$optimalCutoff
opt = optCut$optimalCutoff
table(glm.pred1 > opt, valid$cible)
fitted.results <- ifelse(glm.pred1 > opt,1,0)
misClasificError <- mean(fitted.results != valid$cible)
misClasificError
print(paste('Accuracy',1-misClasificError))
plot(predict(model2.step),residuals(model2.step))
abline(h=0,lty=2,col="red")
pr=prediction(glm.pred1, valid$cible)
prf=performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc=performance(pr, measure = "auc")
auc=auc@y.values[[1]]
auc
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, department, latitude, longitude,X,Y,reg))
trainIndex = createDataPartition(learn_arbres$cible,p=0.7, list=FALSE,times=1)
train_arbre = learn_arbres[trainIndex,]
valid_arbre = learn_arbres[-trainIndex,]
New.tree <- rpart(cible~.,data=train_arbre,method="class")
rpart.plot(New.tree, type = 3, clip.right.labs = FALSE, under = TRUE,fallen.leaves=FALSE)
#rpart.rules(New.tree, cover = TRUE)
plotcp(New.tree)
printcp(New.tree)
tree.pred <- predict(New.tree,newdata = valid_arbre ,type="class")
table(tree.pred, valid_arbre$cible)
pred = prediction(as.numeric(tree.pred), as.numeric(valid_arbre$cible))
roc = performance(pred, measure="tpr", x.measure="fpr")
plot(roc, col="orange", lwd=2)
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
auc=performance(pred, measure = "auc")
auc=auc@y.values[[1]]
auc
New.tree2 <- rpart(cible~.,data=train_arbre,method="class",control = rpart.control(minsplit = 2, cp = 0.0001))
rpart.plot(New.tree2, type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)
View(train_arbre)
#set.seed(1234)
#rforest=randomForest(ciblenum~.,data=train_arbre, na.action=na.omit)
#print(rforest)
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, department, latitude, longitude,X,Y,reg,rev.mv,revmv.cible,csp))
trainIndex = createDataPartition(learn_arbres$cible,p=0.7, list=FALSE,times=1)
train_arbre = learn_arbres[trainIndex,]
valid_arbre = learn_arbres[-trainIndex,]
New.tree <- rpart(cible~.,data=train_arbre,method="class")
New.tree <- rpart(cible~.,data=train_arbre,method="class")
rpart.plot(New.tree, type = 3, clip.right.labs = FALSE, under = TRUE,fallen.leaves=FALSE)
#rpart.rules(New.tree, cover = TRUE)
plotcp(New.tree)
printcp(New.tree)
tree.pred <- predict(New.tree,newdata = valid_arbre ,type="class")
table(tree.pred, valid_arbre$cible)
pred = prediction(as.numeric(tree.pred), as.numeric(valid_arbre$cible))
roc = performance(pred, measure="tpr", x.measure="fpr")
plot(roc, col="orange", lwd=2)
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
auc=performance(pred, measure = "auc")
auc=auc@y.values[[1]]
auc
New.tree2 <- rpart(cible~.,data=train_arbre,method="class",control = rpart.control(minsplit = 2, cp = 0.0001))
rpart.plot(New.tree2, type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)
View(train_arbre)
#set.seed(1234)
#rforest=randomForest(ciblenum~.,data=train_arbre, na.action=na.omit)
#print(rforest)
set.seed(123) ## pour pouvoir le reproduire
learn3 = subset(learn, select=-c(insee.code,f_name,last.name,commune,latitude, longitude,X,Y,reg,revenue,rev.mv,revmv.cible,csp, age.tra))
trainIndex = createDataPartition(learn3$cible,p=0.7, list=FALSE,times=1)
train = learn3[trainIndex,]
valid = learn3[-trainIndex,]
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, department, latitude, longitude,X,Y,reg,rev.mv,revmv.cible,csp, age.tra))
trainIndex = createDataPartition(learn_arbres$cible,p=0.7, list=FALSE,times=1)
train_arbre = learn_arbres[trainIndex,]
valid_arbre = learn_arbres[-trainIndex,]
View(train_arbre)
#set.seed(1234)
#rforest=randomForest(ciblenum~.,data=train_arbre, na.action=na.omit)
#print(rforest)
View(train_arbre)
set.seed(1234)
rforest=randomForest(ciblenum~.,data=train_arbre)
View(train_arbre)
set.seed(1234)
rforest=randomForest(cible~.,data=train_arbre)
View(train_arbre)
set.seed(1234)
rforest=randomForest(as.factor(ciblenum)~.,data=train_arbre,na.action=na.omit)
#print(rforest)
View(train_arbre)
set.seed(1234)
rforest=randomForest(as.factor(ciblenum)~.,data=train_arbre,na.action=na.omit)
print(rforest)
print(rforest)
setwd("C:/Users/Marlène/Desktop/MachineLearning/Projet-ML")
setwd("C:/Users/Marlène/Desktop/MachineLearning/Projet-ML")
set.seed(1234)
rforest=randomForest(as.factor(ciblenum)~.,data=train_arbre,na.action=na.omit)
print(rforest)
set.seed(123) ## pour pouvoir le reproduire
learn3 = subset(learn, select=-c(insee.code,f_name,last.name,commune,latitude, longitude,X,Y,reg,revenue,rev.mv,revmv.cible,csp, age.tra, ciblenum))
trainIndex = createDataPartition(learn3$cible,p=0.7, list=FALSE,times=1)
train = learn3[trainIndex,]
valid = learn3[-trainIndex,]
model1=glm(cible~., data=train,family = binomial(logit))
model2=glm(cible ~ 1, data=train,family = binomial(logit))
model2.step=stepAIC(model2, direction = "both", scope=list(upper=model1,lower=model2),trace = FALSE)
summary(model2.step)
glm.pred1 <- predict(model2.step, newdata = valid, type = "response")
table(glm.pred1 > 0.5, valid$cible)
mean(abs((glm.pred1 > 0.5) - valid$cible), na.rm = T)
optCut <- optimalCutoff(valid$cible, glm.pred1, optimiseFor = "misclasserror",returnDiagnostics = TRUE)
optCut$optimalCutoff
fitted.results <- ifelse(glm.pred1 > opt,1,0)
misClasificError <- mean(fitted.results != valid$cible)
misClasificError
print(paste('Accuracy',1-misClasificError))
plot(predict(model2.step),residuals(model2.step))
abline(h=0,lty=2,col="red")
pr=prediction(glm.pred1, valid$cible)
prf=performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc=performance(pr, measure = "auc")
auc=auc@y.values[[1]]
auc
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, cible, latitude, longitude,X,Y,reg,rev.mv,revmv.cible,csp, age.tra))
trainIndex = createDataPartition(learn_arbres$cible,p=0.7, list=FALSE,times=1)
train_arbre = learn_arbres[trainIndex,]
valid_arbre = learn_arbres[-trainIndex,]
New.tree <- rpart(ciblenum~.,data=train_arbre,method="class")
rpart.plot(New.tree, type = 3, clip.right.labs = FALSE, under = TRUE,fallen.leaves=FALSE)
#rpart.rules(New.tree, cover = TRUE)
plotcp(New.tree)
printcp(New.tree)
tree.pred <- predict(New.tree,newdata = valid_arbre ,type="class")
table(tree.pred, valid_arbre$cible)
auc=performance(pred, measure = "auc")
auc=auc@y.values[[1]]
auc
New.tree2 <- rpart(ciblenum~.,data=train_arbre,method="class",control = rpart.control(minsplit = 2, cp = 0.0001))
rpart.plot(New.tree2, type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)
set.seed(1234)
rforest=randomForest(as.factor(ciblenum)~.,data=train_arbre,na.action=na.omit)
set.seed(1234)
rforest=randomForest(ciblenum~.,data=train_arbre,na.action=na.omit)
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, ciblenum, latitude, longitude,X,Y,reg,rev.mv,revmv.cible,csp, age.tra))
trainIndex = createDataPartition(learn_arbres$cible,p=0.7, list=FALSE,times=1)
train_arbre = learn_arbres[trainIndex,]
valid_arbre = learn_arbres[-trainIndex,]
New.tree <- rpart(cible~.,data=train_arbre,method="class")
rpart.plot(New.tree, type = 3, clip.right.labs = FALSE, under = TRUE,fallen.leaves=FALSE)
#rpart.rules(New.tree, cover = TRUE)
plotcp(New.tree)
printcp(New.tree)
tree.pred <- predict(New.tree,newdata = valid_arbre ,type="class")
table(tree.pred, valid_arbre$cible)
pred = prediction(as.numeric(tree.pred), as.numeric(valid_arbre$cible))
roc = performance(pred, measure="tpr", x.measure="fpr")
plot(roc, col="orange", lwd=2)
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
New.tree2 <- rpart(cible~.,data=train_arbre,method="class",control = rpart.control(minsplit = 2, cp = 0.0001))
rpart.plot(New.tree2, type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)
set.seed(1234)
rforest=randomForest(cible~.,data=train_arbre,na.action=na.omit)
set.seed(1234)
rforest=randomForest(as.factor(cible)~.,data=train_arbre,na.action=na.omit)
set.seed(1234)
train_arbre$cible[train_arbre$cible == "FALSE"]=-1
train_arbre$cible[train_arbre$cible == "TRUE"]=1
train_arbre$cible=as.factor(train_arbre$cible)
rforest=randomForest(cible~.,data=train_arbre,na.action=na.omit)
train_arbre
set.seed(1234)
train_arbre$cible[train_arbre$cible == "FALSE"]=-1
train_arbre$cible[train_arbre$cible == "TRUE"]=1
train_arbre$cible=as.factor(train_arbre$cible)
View(train_arbre)
rforest=randomForest(cible~.,data=train_arbre,na.action=na.omit)
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, ciblenum, latitude,departement, longitude,X,Y,reg,rev.mv,revmv.cible,csp, age.tra))
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, ciblenum, latitude,dept, longitude,X,Y,reg,rev.mv,revmv.cible,csp, age.tra))
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, ciblenum, latitude, department, longitude,X,Y,reg,rev.mv,revmv.cible,csp, age.tra))
trainIndex = createDataPartition(learn_arbres$cible,p=0.7, list=FALSE,times=1)
train_arbre = learn_arbres[trainIndex,]
valid_arbre = learn_arbres[-trainIndex,]
New.tree <- rpart(cible~.,data=train_arbre,method="class")
rpart.plot(New.tree, type = 3, clip.right.labs = FALSE, under = TRUE,fallen.leaves=FALSE)
#rpart.rules(New.tree, cover = TRUE)
plotcp(New.tree)
printcp(New.tree)
tree.pred <- predict(New.tree,newdata = valid_arbre ,type="class")
table(tree.pred, valid_arbre$cible)
pred = prediction(as.numeric(tree.pred), as.numeric(valid_arbre$cible))
roc = performance(pred, measure="tpr", x.measure="fpr")
plot(roc, col="orange", lwd=2)
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
auc=performance(pred, measure = "auc")
auc=auc@y.values[[1]]
auc
New.tree2 <- rpart(cible~.,data=train_arbre,method="class",control = rpart.control(minsplit = 2, cp = 0.0001))
rpart.plot(New.tree2, type = 3, clip.right.labs = FALSE, branch = .3, under = TRUE)
set.seed(1234)
train_arbre$cible[train_arbre$cible == "FALSE"]=-1
train_arbre$cible[train_arbre$cible == "TRUE"]=1
train_arbre$cible=as.factor(train_arbre$cible)
View(train_arbre)
rforest=randomForest(cible~.,data=train_arbre,na.action=na.omit)
print(rforest)
print(rforest)
set.seed(123) ## pour pouvoir le reproduire
learn_arbres = subset(learn, select=-c(insee.code,f_name,last.name,commune, ciblenum, latitude, department, longitude,X,Y,reg,rev.mv,revmv.cible,csp, age.tra))
trainIndex = createDataPartition(learn_arbres$cible,p=0.7, list=FALSE,times=1)
train_arbre = learn_arbres[trainIndex,]
valid_arbre = learn_arbres[-trainIndex,]
set.seed(1234)
#train_arbre$cible[train_arbre$cible == "FALSE"]=-1
#train_arbre$cible[train_arbre$cible == "TRUE"]=1
#train_arbre$cible=as.factor(train_arbre$cible)
rforest=randomForest(cible~.,data=train_arbre,na.action=na.omit)
print(rforest)
rforest
set.seed(1234)
train_arbre$cible[train_arbre$cible == "FALSE"]=-1
train_arbre$cible[train_arbre$cible == "TRUE"]=1
train_arbre$cible=as.factor(train_arbre$cible)
rforest=randomForest(cible~.,data=train_arbre,na.action=na.omit)
print(rforest)
rforest
# table de confusion
rforest$confusion
#importance des variable
rforest$importance[order(rforest$importance[,1],decreasing=TRUE),]
varImpPlot(rforest)
barplot(t(rforest$importance,last=2))
# table de confusion
#rforest$confusion
#importance des variable
rforest$importance[order(rforest$importance[,1],decreasing=TRUE),]
varImpPlot(rforest)
#barplot(t(rforest$importance,last=2))
#analyse graphique pour choisir ntree
plot(rforest$err.rate[,1],type='l',xlab="nb arbres",ylab="erreur OOB")
#marge de chaque individu
margin(rforest)
# table de confusion
#rforest$confusion
#importance des variable
#rforest$importance[order(rforest$importance[,1],decreasing=TRUE),]
varImpPlot(rforest)
#barplot(t(rforest$importance,last=2))
#analyse graphique pour choisir ntree
plot(rforest$err.rate[,1],type='l',xlab="nb arbres",ylab="erreur OOB")
#marge de chaque individu
margin(rforest)
# table de confusion
#rforest$confusion
#importance des variable
#rforest$importance[order(rforest$importance[,1],decreasing=TRUE),]
varImpPlot(rforest)
#barplot(t(rforest$importance,last=2))
#analyse graphique pour choisir ntree
plot(rforest$err.rate[,1],type='l',xlab="nb arbres",ylab="erreur OOB")
#marge de chaque individu
margin(rforest)
# table de confusion
#rforest$confusion
#importance des variable
#rforest$importance[order(rforest$importance[,1],decreasing=TRUE),]
varImpPlot(rforest)
barplot(t(rforest$importance,last=2))
# table de confusion
#rforest$confusion
#importance des variable
#rforest$importance[order(rforest$importance[,1],decreasing=TRUE),]
varImpPlot(rforest)
barplot(t(rforest$importance,las=2))
# table de confusion
#rforest$confusion
#importance des variable
#rforest$importance[order(rforest$importance[,1],decreasing=TRUE),]
varImpPlot(rforest)
barplot(t(rforest$importance))
#analyse graphique pour choisir ntree
plot(rforest$err.rate[,1],type='l',xlab="nb arbres",ylab="erreur OOB")
valid_arbre$cible[valid_arbre$cible == "FALSE"]=-1
valid_arbre$cible[valid_arbre$cible == "TRUE"]=1
valid_arbre$cible=as.factor(valid_arbre$cible)
rforest.pred=predict(rforest,newdata=valid_arbre)
table(rforest.pred,valid_arbre$cible)
valid_arbre$cible[valid_arbre$cible == "FALSE"]=-1
valid_arbre$cible[valid_arbre$cible == "TRUE"]=1
valid_arbre$cible=as.factor(valid_arbre$cible)
rforest.pred=predict(rforest,newdata=valid_arbre)
table(rforest.pred,valid_arbre$cible)
mean(as.integer(rforest.pred!=valid_arbre$cible))
valid_arbre$cible[valid_arbre$cible == "FALSE"]=-1
valid_arbre$cible[valid_arbre$cible == "TRUE"]=1
valid_arbre$cible=as.factor(valid_arbre$cible)
rforest.pred=predict(rforest,newdata=valid_arbre)
table(rforest.pred,valid_arbre$cible)
mean(as.integer(rforest.pred!=valid_arbre$cible))
rf.pred = prediction(as.numeric(rforest.pred), as.numeric(valid_arbre$cible))
rf.roc = performance(rf.pred, measure="tpr", x.measure="fpr")
plot(roc, col="orange", lwd=2)
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
rf.auc=performance(rf.pred, measure = "auc")
rf.auc=auc@y.values[[1]]
rf.auc=performance(rf.pred, measure = "auc")
rf.auc=rf.auc@y.values[[1]]
rf.auc
